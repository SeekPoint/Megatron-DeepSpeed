https://developer.aliyun.com/article/1247831
处理数据
本指南使用1GB 79K-record的JSON格式的OSCAR数据集。
1.执行以下命令，下载数据集。
wget https://huggingface.co/bigscience/misc-test-data/resolve/main/stas/oscar-1GB.jsonl.xz
wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json
wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt
2.执行以下命令，解压数据集。
xz -d oscar-1GB.jsonl.xz

(gh_Megatron-DeepSpeed) amd00@MZ32-00:~/yk_repo/Megatron-DeepSpeed$
python3 tools/preprocess_data.py --input oscar-1GB.jsonl --output-prefix meg-gpt2 --vocab gpt2-vocab.json --dataset-impl mmap --tokenizer-type GPT2BPETokenizer --merge-file gpt2-merges.txt --append-eod --workers 8



(gh_Megatron-DeepSpeed) amd00@MZ32-00:~/yk_repo/Megatron-DeepSpeed$ mkdir data
(gh_Megatron-DeepSpeed) amd00@MZ32-00:~/yk_repo/Megatron-DeepSpeed$ mv meg
megatron/                   meg-gpt2_text_document.bin  meg-gpt2_text_document.idx
(gh_Megatron-DeepSpeed) amd00@MZ32-00:~/yk_repo/Megatron-DeepSpeed$ mv gpt2-
gpt2-merges.txt  gpt2-vocab.json
(gh_Megatron-DeepSpeed) amd00@MZ32-00:~/yk_repo/Megatron-DeepSpeed$ mv gpt2-* data/
(gh_Megatron-DeepSpeed) amd00@MZ32-00:~/yk_repo/Megatron-DeepSpeed$ mv meg-gpt2_text_document* data/
==============================

python -m pip install numpy==1.22 -i https://pypi.org/simple


(gh_Megatron-DeepSpeed) amd00@MZ32-00:~/llm_dev/Megatron-DeepSpeed/data$ ll
total 1051672
drwxrwxr-x  2 amd00 amd00       4096 6月  27 01:50 ./
drwxrwxr-x 12 amd00 amd00       4096 6月  27 01:54 ../
-rw-rw-r--  1 amd00 amd00     456318 2月  18  2019 gpt2-merges.txt
-rw-rw-r--  1 amd00 amd00    1042301 2月  18  2019 gpt2-vocab.json
-rw-rw-r--  1 amd00 amd00 1075395068 7月  25  2021 oscar-1GB.jsonl
(gh_Megatron-DeepSpeed) amd00@MZ32-00:~/llm_dev/Megatron-DeepSpeed/data$ mv * ../
(gh_Megatron-DeepSpeed) amd00@MZ32-00:~/llm_dev/Megatron-DeepSpeed/data$ cd ..
(gh_Megatron-DeepSpeed) amd00@MZ32-00:~/llm_dev/Megatron-DeepSpeed$ python3 tools/preprocess_data.py     --input oscar-1GB.jsonl     --output-prefix meg-gpt2     --vocab gpt2-vocab.json     --dataset-impl mmap     --tokezer-type GPT2BPETokenizer     --merge-file gpt2-merges.txt     --append-eod     --workers 8
[2023-06-27 02:05:51,790] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Opening oscar-1GB.jsonl
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> building GPT2BPETokenizer tokenizer ...
。。。
> building GPT2BPETokenizer tokenizer ...
Vocab size: 50257
Output prefix: meg-gpt2
Time to startup: 0.09256768226623535
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
。。。。
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Processed 100 documents (194.43846172032653 docs/s, 2.1531947205949367 MB/s).
。。。。
Processed 79000 documents (1538.6648463165745 docs/s, 19.861539227950722 MB/s).
(gh_Megatron-DeepSpeed) amd00@MZ32-00:~/llm_dev/Megatron-DeepSpeed$


5.执行以下命令，将处理好的数据移动到data目录下。
mv meg-gpt2* ./data
mv gpt2* ./data
