【Megatron-DeepSpeed】张量并行工具代码mpu详解(一)：并行环境初始化

https://zhuanlan.zhihu.com/p/619914112

Megatron-DeepSpeed是DeepSpeed版本的NVIDIA Megatron-LM。
像BLOOM、GLM-130B等主流大模型都是基于Megatron-DeepSpeed开发的。
这里以BLOOM版本的Megetron-DeepSpeed为例，介绍其张量并行代码mpu的细节(位于megatron/mpu下)。

理解该部分的代码需要对张量并行的原理以及集合通信有一定的理解，可以看文章：

白强伟：【深度学习】【分布式训练】Collective通信操作及Pytorch示例
白强伟：【深度学习】【分布式训练】一文捋顺千亿模型训练技术：流水线并行、张量并行和3D并行
白强伟：【深度学习】【分布式训练】DeepSpeed：AllReduce与ZeRO-DP

阅读建议：
1. 本文仅会解析核心代码，并会不介绍所有代码；
2. 本文会提供一些测试脚本来展现各部分代码的功能；
3. 建议实际动手实操来加深理解；
4. 建议对Collective通信以及分布式模型训练有一定理解，再阅读本文；

一、总览
mpu目录下核心文件有：

    initialize.py：负责数据并行组、张量并行组和流水线并行组的初始化，以及获取与各类并行组相关的信息；
    data.py：实现张量并行中的数据广播功能；
    cross_entropy.py：张量并行版本的交叉熵；
    layers.py：并行版本的Embedding层，以及列并行线性层和行并行线性层；
    mappings.py：用于张量并行的通信操作；

二、初始化原理
Megatron-Deepspeed框架能够支持3D并行，而3D并行中显卡如何分配成不同的组等工作就是由mpu目录下的initialize.py完成的。
01.webp

假设有两个节点Node1和Node2，每个节点有8个GPU，共计16个GPU。16个GPU的编号分别为Rank0、Rank1、...、Rank15。
此外，假设用户设置流水线并行度为4，张量并行度为2。

流水线并行
    流水线并行会将整个模型划分为4份，这里称为sub_model_1至sub_model_4。每连续的4张GPU负责一个sub_model。
    即上图右上角中，相同颜色的GPU负责相同的sub_model。
    每个流水线并行组对应一个完整的模型，最终的流水线并行组：

    [Rank0, Rank4, Rank8, Rank12],

    [Rank1, Rank5, Rank9, Rank13],

    [Rank2, Rank6, Rank10, Rank14],

    [Rank3, Rank7, Rank11, Rank15],

张量并行
    张量并行会针对流水线并行中的sub_model来进行张量的拆分。
    即Rank0和Rank1负责一份sub_model_1，Rank2和Rank3负责另一份sub_model_1；
    Rank4和Rank5负责sub_model_2，Rank6和Rank7负责另一份sub_model_2；以此类推。
    上图右下角中，绿色线条表示单个张量并行组，每个张量并行组都共同负责某个具体的sub_model。
    最终的张量并行组：
    [Rank0, Rank1], [Rank2, Rank3],

    [Rank4, Rank5], [Rank6, Rank7],

    [Rank8, Rank9], [Rank10, Rank11],

    [Rank12, Rank13], [Rank14, Rank15],

数据并行
    数据并行的目的是要保证并行中的相同模型参数读取相同的数据。
    经过流水线并行和张量并行后，Rank0和Rank2负责相同的模型参数，所以Rank0和Rank2是同一个数据并行组。
    上图左上角中的红色线条表示数据并行组。
    [Rank0, Rank2], [Rank1, Rank3],

    [Rank4, Rank6], [Rank5, Rank7],

    [Rank8, Rank10], [Rank9, Rank11],

    [Rank12, Rank14], [Rank13, Rank15],

三、初始化代码
这里不对代码的所有细节进行解析。理解上面的原理后，细节部分只要花些时间即可弄明白。
这里仅对整体的代码结构或者某些有代表性的函数进行说明。

总的来说，初始化的目标就是要赋予下面这些变量具体的值。

    # Intra-layer model parallel group that the current rank belongs to.
    _TENSOR_MODEL_PARALLEL_GROUP = None
    # Inter-layer model parallel group that the current rank belongs to.
    _PIPELINE_MODEL_PARALLEL_GROUP = None
    # Model parallel group (both intra- and pipeline) that the current rank belongs to.
    _MODEL_PARALLEL_GROUP = None
    # Embedding group.
    _EMBEDDING_GROUP = None
    # Data parallel group that the current rank belongs to.
    _DATA_PARALLEL_GROUP = None

    _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK = None
    _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = None

    # These values enable us to change the mpu sizes on the fly.
    _MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE = None
    _MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = None
    _MPU_TENSOR_MODEL_PARALLEL_RANK = None
    _MPU_PIPELINE_MODEL_PARALLEL_RANK = None

    # A list of global ranks for each pipeline group to ease calculation of the source
    # rank when broadcasting from the first or last pipeline stage
    _PIPELINE_GLOBAL_RANKS = None

初始化功能的核心是函数 initialize_model_parallel，
其主要的参数就是用户指定的张量并行数tensor_model_parallel_size_以及流水线并行数pipeline_model_parallel_size_。
基于这两个用户参数来计算出各种分组。

    def initialize_model_parallel(tensor_model_parallel_size_=1,
                                  pipeline_model_parallel_size_=1,
                                  virtual_pipeline_model_parallel_size_=None):
    ...
其余的函数主要功能是：获得初始化状态、获得当前rank所在的组、获得当前进行所在特定组里的rank等。
这里功能会在后续的测试代码里展示具体的应用。

此外，官方代码中的函数destroy_model_parallel中应该是存在bug，下面是可正常运行的版本：

    def destroy_model_parallel():
        """Set the groups to none."""
        global _TENSOR_MODEL_PARALLEL_GROUP
        _TENSOR_MODEL_PARALLEL_GROUP = None
        global _PIPELINE_MODEL_PARALLEL_GROUP
        _PIPELINE_MODEL_PARALLEL_GROUP = None
        global _DATA_PARALLEL_GROUP
        _DATA_PARALLEL_GROUP = None
        # 以下为新增
        global _MODEL_PARALLEL_GROUP
        _MODEL_PARALLEL_GROUP = None
        global _EMBEDDING_GROUP
        _EMBEDDING_GROUP = None

四、测试代码
1. 测试设置
    测试使用8张GPU；
    张量并行度为2，流水线并行度为2；
    依据上面介绍的原理，流水线并行组为：[Rank0, Rank4],[Rank1, Rank5],[Rank2, Rank6], [Rank3, Rank7]
    依据上面介绍的原理，张量并行组为：[Rank0, Rank1],[Rank2, Rank3],[Rank4,Rank5],[Rank6,Rank7]
    依据上面介绍的原理，数据并行组为: [Rank0, Rank2],[Rank1, Rank3],[Rank4,Rank6],[Rank5,Rank7]

2. 辅助代码
这里的辅助代码仍然使用原始单元测试的代码，即/megatron/mpu/tests/commons.py。下面会对这些代码进行简单的注释。
.....

3. 测试代码
3.1 测试代码
为了展示各个函数的功能，这里并不使用原始项目中的测试代码，而是单独撰写的代码。

# test_initialize.py

3.2 启动脚本
deepspeed test_initialize.py

3.3 输出结果

上图是Rank5的相关信息，符合前面对于其张量并行组、流水线并行组和数据并行组的分析。
02.png


相关文章：

白强伟：【Megatron-DeepSpeed】张量并行工具代码mpu详解(四)：张量并行版Embedding层及交叉熵的实现及测试

白强伟：【Megatron-DeepSpeed】张量并行工具代码mpu详解(三)：张量并行层的实现及测试

白强伟：【Megatron-DeepSpeed】张量并行工具代码mpu详解(二)：Collective通信操作的封装mappings

白强伟：【Megatron-DeepSpeed】张量并行工具代码mpu详解(一)：并行环境初始化

白强伟：【深度学习】【分布式训练】一文捋顺千亿模型训练技术：流水线并行、张量并行和3D并行

白强伟：【自然语言处理】【大模型】DeepSpeed+Transformers：简单快捷上手百亿参数模型微调

白强伟：【深度学习】【分布式训练】DeepSpeed：AllReduce与ZeRO-DP

白强伟：【深度学习】混合精度训练与显存分析

白强伟：【自然语言处理】【大模型】大语言模型BLOOM推理工具测试

白强伟：【自然语言处理】【大模型】BLOOM：一个176B参数且可开放获取的多语言模型

编辑于 2023-04-22 10:50・IP 属地北京





瘸腿牛蛙妈
请问这部分的初始化可以用于并行推理吗

09-17 · IP 属地北京
​回复
​喜欢
momo
momo
“相同模型参数读取相同的数据”，这个似乎有问题，应该是，相同模型参数读取不同数据，最简单的只有DP的情况，
每台机器上是相同的模型参数，接收的是不同的数据。

07-14 · IP 属地北京
​回复
​喜欢
lRobert
lRobert
我也感觉是这样子的，“相同模型参数读取相同的数据”，给我搞迷了

08-02 · IP 属地中国香港
​回复
​喜欢
锄禾日当午
锄禾日当午
你好，有个疑问请教下，“流水线并行会将整个模型划分为4份，这里称为sub_model_1至sub_model_4。
每连续的4张GPU负责一个sub_model”，这里表述是不是有点问题？
应该是每条流水线负责一个sub_model，且每两张GPU 负责一条流水线？DP之间是独立的，可以只看一台机器。

07-05 · IP 属地北京
​回复
​喜欢
stay
stay
这里的sub_model指的是模型分成的四个部分之一。
所以，连续4张GPU分的都是模型的同一部分，不是流水线组，流水线组则是指的是拼成完整模型的不同rank。

08-28 · IP 属地安徽
​回复
​喜欢
明君非
明君非
​
代码有git?

06-08 · IP 属地浙江
​回复
​喜欢
自言自梦
自言自梦

您好，想请教几个问题：
1 megatron的mpu是提供tensor并行的部分，但是deepspeed也提供了tensor并行，为什么还要结合megatron呢。

2 我看deepspeed源码的训练部分，只有moe的tensor并行是现成的，但是moe和现在的gpt不同，
似乎inference部分也有tensor并行的，但是对于我们GPT的训练来说等于依然没有可以直接使用的并行模块对么。

05-15 · IP 属地浙江
​回复
​喜欢
suyako
suyako
感觉deepspeed是数据并行，我对此的区分理解是张量并行复制数据，数据并行复制模型。
如果看deepspeed原理的话它虽然对模型参数和optimizer state做了切分，但实际运行时gpu上的模型参数还是要相互复制（即复制模型），
但是不同的gpu跑的数据是不同的（即不复制数据）。

05-26 · IP 属地上海
​回复
​1
mactavish
mactavish

IseesI
deepspeed zero3和megatron能一起用吗

08-20 · IP 属地北京
​回复
​喜欢
IseesI
IseesI
我理解的，Deepspeed stage3 也对 weight 做了切分，跟 tensor parallel 比较像，但是它本质上还是 data parallel 的思路。
可能的实现方式是，在计算当前层之前，需要先把这一层的权重从其他 gpu 上gather起来再计算，
所以每个 GPU 上可以认为是一个完整的 model（不涉及对输入X做切分之类的操作），
只不过 deepspeed会帮助实现 weight的切分以及用的时候的（预）收集等操作，但这也导致通信开销很大，不如 megatron 效率高。

06-07 · IP 属地新加坡
​回复
​喜欢
锐昂
锐昂
IseesI
不对stage2、3对梯度的切分和张量并行完全不一样
08-16 · IP 属地北京
​回复
​喜欢
IseesI
IseesI
锐昂
嗯，我说的是weight 切分后的计算；对梯度的切分，deepspeed 应该有很多自己的优化

08-17 · IP 属地新加坡
​回复
​喜欢
2cat
2cat
看了半天没看懂，能不能先介绍一下mpu是什么？是deepspeed的功能还是Megatron的功能？网上查到的mpu有可能是很多东西的缩写

05-08 · IP 属地安徽
​回复
​喜欢
Putrip
Putrip
model parallel utility

08-10 · IP 属地北京
​回复
​1
白强伟
白强伟
作者
​
文章标题也写的很明白《张量并行》
05-08 · IP 属地北京
​回复
​喜欢
白强伟
白强伟
作者
​
是megatron的功能～至于没看懂的话，说明知识储备不够，建议把上面推荐的文章先看明白
05-08 · IP 属地北京