【Megatron-DeepSpeed】张量并行工具代码mpu详解(四)：张量并行版Embedding层及交叉熵的实现及测试

https://zhuanlan.zhihu.com/p/623930146

张量并行版Embedding层及交叉熵的实现及测试

Megatron-DeepSpeed是DeepSpeed版本的NVIDIA Megatron-LM。像BLOOM、GLM-130B等主流大模型都是基于Megatron-DeepSpeed开发的。
这里以BLOOM版本的Megetron-DeepSpeed为例，介绍其模型并行代码mpu的细节(位于megatron/mpu下)。

理解该部分的代码需要对模型并行的原理以及集合通信有一定的理解，可以看文章：

白强伟：【深度学习】【分布式训练】Collective通信操作及Pytorch示例
白强伟：【深度学习】【分布式训练】一文捋顺千亿模型训练技术：流水线并行、张量并行和3D并行
白强伟：【深度学习】【分布式训练】DeepSpeed：AllReduce与ZeRO-DP


 强烈建议阅读，不然会影响本文的理解：

【Megatron-DeepSpeed】张量并行工具代码mpu详解(一)：并行环境初始化
【Megatron-DeepSpeed】张量并行工具代码mpu详解(二)：Collective通信操作的封装mappings
【Megatron-DeepSpeed】张量并行工具代码mpu详解(三)：张量并行层的实现及测试
阅读建议：
1. 本文仅会解析核心代码，并会不介绍所有代码；
2. 本文会提供一些测试脚本来展现各部分代码的功能；
3. 建议实际动手实操来加深理解；
4. 建议对Collective通信以及分布式模型训练有一定理解，再阅读本文；

一、总览
 mpu目录下核心文件有：

initialize.py：负责数据并行组、张量并行组和流水线并行组的初始化，以及获取与各类并行组相关的信息；
data.py：实现张量并行中的数据广播功能；
cross_entropy.py：张量并行版本的交叉熵；
layers.py：并行版本的Embedding层，以及列并行线性层和行并行线性层；
mappings.py：用于张量并行的通信操作；
二、张量并行版Embedding层

 Embedding层本质就是一个查找表。
 如上图所示，张量并行版embedding层就是将完整的embedding层，在vocab的维度切分。
 张量并行组中的每个进程仅持有部分embedding层。

08.webp
1. 实现代码
 这里直接在原始的文件(megatron/mpu/layers.py)中，添加一个自定义的并行版Embedding层。
 其与原始版完全相同，仅添加了一些输出来展示整个过程。

    # layers.py
    class MyVocabParallelEmbedding(torch.nn.Module):
        def __init__(self, num_embeddings, embedding_dim,
    ....

2. 测试脚本
 实验设置为：张量并行度为2，流水线并行度也为2。测试脚本比较简单，直接调用上面实现的MyVocabParallelEmbedding。

# test_embedding.py
....


三、张量并行版交叉熵
 我们以自然语言模型为例，展示交叉熵的计算原理。

yknote---截图 08.png

 mpu代码中的交叉熵实现基本上遵循上面的分析，仅是添加了batch size和seq_length维度，但核心思想不变。

1. 实现代码
 同样，也是在原始文件(megatron/mpu/cross_entropy.py)中，添加一个自定义的并行版交叉熵。
 该实现与原版完全相同，仅添加了一些输出来展示整个过程。

# cross_entropy.py
class _MyVocabParallelCrossEntropy(torch.autograd.Function):
。。。

相关文章：

白强伟：【Megatron-DeepSpeed】张量并行工具代码mpu详解(四)：张量并行版Embedding层及交叉熵的实现及测试

白强伟：【Megatron-DeepSpeed】张量并行工具代码mpu详解(三)：张量并行层的实现及测试

白强伟：【Megatron-DeepSpeed】张量并行工具代码mpu详解(二)：Collective通信操作的封装mappings

白强伟：【Megatron-DeepSpeed】张量并行工具代码mpu详解(一)：并行环境初始化

白强伟：【深度学习】【分布式训练】一文捋顺千亿模型训练技术：流水线并行、张量并行和3D并行

白强伟：【自然语言处理】【大模型】DeepSpeed+Transformers：简单快捷上手百亿参数模型微调

白强伟：【深度学习】【分布式训练】DeepSpeed：AllReduce与ZeRO-DP

白强伟：【深度学习】混合精度训练与显存分析

白强伟：【自然语言处理】【大模型】大语言模型BLOOM推理工具测试

白强伟：【自然语言处理】【大模型】BLOOM：一个176B参数且可开放获取的多语言模型

编辑于 2023-04-22 10:51・IP 属地北京